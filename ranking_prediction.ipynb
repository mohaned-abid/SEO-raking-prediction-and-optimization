{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Valid SEMrush API Key needed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from helper_classes.semrush.semrush_api import SEMRushAPI\n",
    "\n",
    "# your SEMRush API Key\n",
    "api_key = ''\n",
    "# SEMRush Keyword Database used (Complete list https://www.semrush.com/api-analytics/)\n",
    "database = 'de'\n",
    "\n",
    "sr = SEMRushAPI(api_key, database=database)\n",
    "reg_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Domain to optimize & competitors (more competitors=better results)\n",
    "main_domain = ['www.barf-alarm.de']\n",
    "competitor_domains = ['barf-factory.de', 'www.barfers-wellfood.de',\n",
    "                      'www.barfinfo.de', 'www.barfkoeln.de', 'www.barfshop-koeln.de',\n",
    "                      'www.barfshop.de']\n",
    "base_domains = main_domain + competitor_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Information about the Domains (Quelle: SEMRUSH)\n",
    "domain_arr = []\n",
    "for domain in base_domains:\n",
    "    sr_result = sr.get_domain_overview(domain)\n",
    "    if sr_result:\n",
    "        domain_arr.append(sr_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df = pd.DataFrame.from_records(domain_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n",
      "Error reaching SEMrush API: ERROR 122 :: WRONG FORMAT OR EMPTY KEY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all Keywords for the listed Domains (Quelle: SEMRUSH API)\n",
    "# This can take a while an need API Credits\n",
    "# The results of the API will be cached\n",
    "keyword_df = None\n",
    "keyword_df = pd.DataFrame()\n",
    "for domain in base_domains:\n",
    "    res = sr.get_domain_keywords(domain, count=2000)\n",
    "    if res:\n",
    "        kw_df = pd.DataFrame.from_records(res)\n",
    "        kw_df[\"Domain\"] = domain\n",
    "        keyword_df = pd.concat([kw_df, keyword_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CPC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2890\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2891\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CPC'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2bd99a34d1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Convert Strings to numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mkeyword_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CPC\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CPC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mkeyword_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Competition\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Competition\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mkeyword_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Search Volume\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Search Volume\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2973\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2890\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2893\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CPC'"
     ]
    }
   ],
   "source": [
    "# Delete not needed columns\n",
    "if \"Position Difference\" in keyword_df:\n",
    "    del keyword_df[\"Position Difference\"]\n",
    "if \"Previous Position\" in keyword_df:\n",
    "    del keyword_df[\"Previous Position\"]\n",
    "if \"Trends\" in keyword_df:\n",
    "    del keyword_df[\"Trends\"]\n",
    "if \"Traffic (%)\" in keyword_df:\n",
    "    del keyword_df[\"Traffic (%)\"]\n",
    "if \"Traffic Cost (%)\" in keyword_df:\n",
    "    del keyword_df[\"Traffic Cost (%)\"]\n",
    "\n",
    "# Convert Strings to numbers\n",
    "keyword_df[\"CPC\"] = pd.to_numeric(keyword_df[\"CPC\"]).astype(float)\n",
    "keyword_df[\"Competition\"] = pd.to_numeric(keyword_df[\"Competition\"]).astype(float)\n",
    "keyword_df[\"Search Volume\"] = pd.to_numeric(keyword_df[\"Search Volume\"]).astype(float)\n",
    "keyword_df[\"Position\"] = pd.to_numeric(keyword_df[\"Position\"]).astype(float)\n",
    "keyword_df[\"Number of Results\"] = pd.to_numeric(keyword_df[\"Number of Results\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df.sort_values('Search Volume', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each keyword only once\n",
    "keyword_df_unique = keyword_df.drop_duplicates(subset='Keyword', keep=\"last\")\n",
    "\n",
    "# Delete not neded rows\n",
    "del keyword_df_unique[\"Position\"]\n",
    "del keyword_df_unique[\"Url\"]\n",
    "del keyword_df_unique[\"Domain\"]\n",
    "\n",
    "print(\"Count unique Keywords for all Domains: {}\".format(len(keyword_df_unique.index)))\n",
    "keyword_df_unique.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Information about the found urls of the positions\n",
    "# Reduce to only competitors\n",
    "all_urls = keyword_df.drop_duplicates(subset='Url', keep=\"last\")\n",
    "\n",
    "urls_to_crawl_df = all_urls[all_urls['Domain'].isin(base_domains)]\n",
    "urls_to_crawl = urls_to_crawl_df.Url\n",
    "print(\"Count of URLs to crawl: {}\".format(len(urls_to_crawl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a simple crawler to fetch URLs\n",
    "from helper_classes.simple_crawler.simple_crawler import Crawler\n",
    "crawler = Crawler(30) # How much Threads in parallel?\n",
    "\n",
    "# Crawl\n",
    "# Crawl results are cached\n",
    "res = crawler.get_websites(urls_to_crawl)\n",
    "\n",
    "# Build DF from cralwed URLS\n",
    "crawled_df = pd.DataFrame.from_records(res)\n",
    "crawled_df.rename(columns={'url': 'Url'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Title & Content for {} Urls\".format(len(crawled_df.index)))\n",
    "crawled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Keywords and positions with crawled content\n",
    "combined_df = pd.merge(keyword_df, crawled_df, on='Url', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each Title, URL, Content if words of Keyword phrase are in it and save information as float\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from urllib.parse import urlparse\n",
    "stopwords_list = set(stopwords.words('german'))\n",
    "\n",
    "def check_keyword(args):\n",
    "    \n",
    "    words = args[\"Keyword\"].split(\" \")\n",
    "    words_wo_stop = [i for i in words if not i in stopwords_list]\n",
    "    count_kw = len(words_wo_stop)\n",
    "    \n",
    "    found_words_title = 0\n",
    "    found_words_content = 0\n",
    "    found_words_url = 0\n",
    "    found_words_domain = 0\n",
    "    url_found = 0.0\n",
    "    content_found = 0.0\n",
    "    title_found = 0.0\n",
    "    domain_found = 0.0\n",
    "    is_homepage = 0\n",
    "    for word in words_wo_stop:\n",
    "        if str(word) in str(args[\"Domain\"]):\n",
    "            found_words_domain += 1 \n",
    "\n",
    "        path = urlparse(args[\"Url\"]).path\n",
    "        if str(word) in str(path):\n",
    "            found_words_url += 1 \n",
    "        if \"/\" == path:\n",
    "            is_homepage = 1\n",
    "        if str(word) in str(args[\"title\"]):\n",
    "            found_words_title += 1\n",
    "        if str(word) in str(args[\"response_content\"]):\n",
    "            found_words_content += 1\n",
    "    if found_words_url > 0:\n",
    "        url_found = found_words_url / count_kw\n",
    "    if found_words_title > 0:\n",
    "        title_found = found_words_title / count_kw\n",
    "    if found_words_content > 0:\n",
    "        content_found = found_words_content / count_kw\n",
    "    if found_words_content > 0:\n",
    "        domain_found = found_words_domain / count_kw\n",
    "    return pd.Series({'title_found': title_found, \n",
    "                      'content_found': content_found, \n",
    "                      'url_found': url_found, \n",
    "                      'domain_found': domain_found,\n",
    "                      'is_homepage': is_homepage})\n",
    "rated_df = combined_df.join(combined_df.apply(check_keyword, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an ID for each domain and keyword so we can provide the ML Algo the ID and have a representating array\n",
    "domains = list(rated_df.Domain.unique())\n",
    "keywords = list(rated_df.Keyword.unique())\n",
    "def get_domain_id(domain):\n",
    "    return domains.index(domain)\n",
    "def get_keyword_id(keyword):\n",
    "    return keywords.index(keyword)\n",
    "rated_df['Domain_id'] = rated_df['Domain'].apply(get_domain_id)\n",
    "rated_df['Keyword_id'] = rated_df['Keyword'].apply(get_keyword_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schow example entrys for Keyword ID 2\n",
    "rated_df[rated_df[\"Keyword_id\"]==2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the top 40 results\n",
    "# rated_df_top40 = rated_df[rated_df[\"Position\"]<40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows with no response_status (if any)\n",
    "rated_df.dropna(subset=['response_status'], how='all', inplace=True)\n",
    "print(\"{} rows for machine learning\".format(len(rated_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features (which rows should the ml algorithm should consider for learning)\n",
    "features = [\"Domain_id\", \n",
    "            \"content_found\", \n",
    "            \"domain_found\",\n",
    "            \"title_found\", \n",
    "            \"url_found\", \n",
    "            \"Keyword_id\", \n",
    "            \"CPC\", \n",
    "            \"Number of Results\",\n",
    "            \"Search Volume\"]\n",
    "# Define Target (What should be predicted)\n",
    "target = \"Position\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test data\n",
    "train = rated_df.sample(frac=0.8)\n",
    "test = rated_df.loc[~rated_df.index.isin(train.index)]\n",
    "print (\"Train rows: {}\".format(len(train.index)))\n",
    "print (\"Test rows: {}\".format(len(test.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with Data\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "mlc = MLPClassifier(activation = 'relu', random_state=1,nesterovs_momentum=True)\n",
    "loo = LeaveOneOut()\n",
    "pipe = make_pipeline(sc, mlc)\n",
    "\n",
    "# Train the Model and check wich of the Parameters works best\n",
    "parameters = { \"mlpclassifier__hidden_layer_sizes\":[(300,),(500,)],  \n",
    "              \"mlpclassifier__solver\" : ( \"sgd\", \"lbfgs\"), \n",
    "              \"mlpclassifier__max_iter\": [500, 1000, 2000], \n",
    "              \"mlpclassifier__learning_rate_init\":[0.001, 0.1]  }\n",
    "MLPClassifierModel = GridSearchCV(pipe, parameters,n_jobs= -1,cv = 5)\n",
    "MLPClassifierModel.fit(train[features], train[target])\n",
    "\n",
    "# Save Model to file to used it later\n",
    "file = open(\"k_t_o_MLPClassifierModel_10_comp.pkl\", 'wb')\n",
    "pickle.dump(MLPClassifierModel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Model from File (could be skipped if you've trained it just a second before ;) )\n",
    "file = open(\"k_t_o_MLPClassifierModel_10_comp.pkl\", 'rb')\n",
    "MLPClassifierModel = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_df[rated_df[\"Domain\"]==main_domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data to query the Model\n",
    "id = 3538 # just change ID to one of the row ids from above\n",
    "data = [\n",
    "    rated_df.ix[id][\"Domain_id\"],\n",
    "    rated_df.ix[id][\"content_found\"],\n",
    "    rated_df.ix[id][\"domain_found\"],\n",
    "    rated_df.ix[id][\"title_found\"],\n",
    "    rated_df.ix[id][\"url_found\"],\n",
    "    rated_df.ix[id][\"Keyword_id\"],\n",
    "    rated_df.ix[id][\"CPC\"],\n",
    "    rated_df.ix[id][\"Number of Results\"],\n",
    "    rated_df.ix[id][\"Search Volume\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Data we query?\n",
    "def print_data(data):\n",
    "    print(\"Domain: {}\".format(domains[int(data[0])]))\n",
    "    print(\"Keyword: {}\".format(keywords[int(data[5])]))\n",
    "    print(\"Keyword in Content Found? {}\".format(data[1]))\n",
    "    print(\"Keyword in Title Found? {}\".format(data[3]))\n",
    "    print(\"Keyword in Domain Found? {}\".format(data[2]))\n",
    "    print(\"Keyword in URL Found? {}\".format(data[4]))\n",
    "    print(\"CPC: {}\".format(data[6]))\n",
    "    print(\"Number of Results: {}\".format(data[7]))\n",
    "    print(\"Search Volume: {}\".format(data[8]))\n",
    "print_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Position\n",
    "df_to_predict = pd.DataFrame(data = [data], index=[0], columns=features)\n",
    "res = MLPClassifierModel.predict(df_to_predict)\n",
    "print(\"MLPClassifierModel predicted Position:  {}\".format(int(res[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Data for that Keyword\n",
    "modified_data = data\n",
    "data[1] = 1\n",
    "data[3] = 1\n",
    "print_data(modified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Position\n",
    "df_to_predict = pd.DataFrame(data = [modified_data], index=[0], columns=features)\n",
    "res = MLPClassifierModel.predict(df_to_predict)\n",
    "print(\"MLPClassifierModel predicted Position:  {}\".format(int(res[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to predict Positions for all Keywords if title & Description are optimized for one of the Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list with all Keywords and if any with position and found Url\n",
    "predict_df = pd.merge(keyword_df[keyword_df[\"Domain\"]==main_domain][[\"Position\", \"Keyword\", \"Url\"]],keyword_df_unique, on='Keyword', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_id = get_domain_id(main_domain)\n",
    "# Predict Data for each row\n",
    "def optimized_position(args):\n",
    "    try: \n",
    "        keyword_id = get_keyword_id(args[\"Keyword\"])\n",
    "    except:\n",
    "        return pd.Series({'optimized_position': None })\n",
    "    data = [\n",
    "        domain_id,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        keyword_id,\n",
    "        args[\"CPC\"],\n",
    "        args[\"Number of Results\"],\n",
    "        args[\"Search Volume\"]\n",
    "    ]\n",
    "    df_to_predict = pd.DataFrame(data = [data], index=[0], columns=features)\n",
    "    res = MLPClassifierModel.predict(df_to_predict)\n",
    "    return pd.Series({'optimized_position': int(res[0])})\n",
    "\n",
    "final_df = predict_df.join(predict_df.apply(optimized_position, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = final_df.sort_values('Search Volume', ascending=False)[final_df[\"optimized_position\"]<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Keywords where the Model predicts a top 10 Position\n",
    "new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
